{"meta":{"title":"Hexo","subtitle":"","description":"","author":"John Doe","url":"http://example.com","root":"/"},"pages":[],"posts":[{"title":"20221011ML","slug":"20221011ML","date":"2022-10-11T12:51:03.000Z","updated":"2022-10-11T15:35:31.878Z","comments":true,"path":"2022/10/11/20221011ML/","link":"","permalink":"http://example.com/2022/10/11/20221011ML/","excerpt":"","text":"很神奇的是打卡和课堂的ML课程在10.10号刚好同时开始嘿嘿 Task01：概览西瓜书 + 南瓜书第1、2章Chap01 ML的一些基本术语数据集、标记、训练样本、测试样本、正类和负类、分布、模型泛化能力…这些概念虽然很抽象但是带入具体例子就一目了然因此不深究 什么是归纳偏好？对于一组训练样本，在多数情况下有多种模型可供选择，且不同的模型往往对应不同的输出效果，而一个具体的机器学习算法要求：“必须要产生一个模型”以使得对于一个给定的输入，有一个稳定的输出，因此我们必须要选择一个模型，这时候所选择的算法在实际输出时就会表现出某种偏好，称之为归纳偏好。 Ocaam’s razor？由于可以不同模型有不同的偏好，那实际生活中我们需要有一种偏好选择的基准，奥卡姆剃刀为一种常用的、自然科学研究中的基本准则。 NFL定理通过理论可以证明（参见pumpkin book P1），对于二分类的总误差与算法选择并无关联，他们的总体期望性能是相同的，这意味着，对于拥有任何可能分布的庞大数据集来说，用模型估计和随机猜测的误差会几乎相同，但这是否意味着我们设计模型没有意义呢？并非如此~事实上，我们设计模型的初衷总是针对一个特定的任务，而这个任务的数据分布往往是一定的，因此训练的模型往往能够鲁棒地在上面发挥作用。套用机器学习老师的说法：“一切抛开具体问题、具体数据分布的模型性能讨论都是在耍流氓[doge]” Chap02 模型评估与选择概念经验误差、泛化误差、过拟合 经验误差：学习器在训练集上的误差泛化误差：新样本上的误差称为“泛化误差”过拟合：学习能力过于强大，以至于把训练样本所包含的不太一般的特性都学到了 评估方法 可以通过实验测试来对学习器泛化误差进行评估,从而做出选择 way1:hold-out该方法普遍使用于数据集比较大的情况:留出法暴力&amp;&amp;好理解:将数据集划分为两个互斥的集合注意:①留出法本质是一种分层抽样,要严格保证训练样本&#x2F;测试样本的数据分布与原数据集相同②要注意数据集划分比例2:8&#x2F;3:7势必会使训练出模型的性能不同的,选择比例的时候要斟酌 way2:交叉验证法将数据集划分为k个大小相似的互斥子集,每个子集也都同时尽可能保持分布一致性(似乎在k值和样本数相接近时,这一点较难保证),进而进行k次的训练和测试. way3:自助法采用有放回抽样的方法,保证训练集的规模和原数据集D一致,但其中样本并不完备,事实上,D中有36.8%左右样本在采样后数据集中不会出现(一个常用极限) 性能度量 错误率: $E(f;D) &#x3D; \\frac{1}{m}\\sum_{i&#x3D;1}^mII(f(x_i)!&#x3D;y_i)$ 精度: $acc(f;D) &#x3D; 1 - E(f;D)$ 查准率:$P &#x3D; \\frac{TP}{TP + FP}$, 查全率$R &#x3D; \\frac{FN}{FN + TN}$ “P-R曲线” 作图方法：根据学习器的预测结果对样例进行排序，排在在前面的是预测的可能性最大的正例样本，按照这个顺序逐个将样本作为正例进行预测，每次计算出当前的查全率、查准率，并绘制P-R曲线作用：直观显示出学习器在样本总体上的查准率、查全率———最优的P-R曲线曲线下面积为1学习器效果对比：①利用BEP(Break-Even Point)就是在P-R图上做一条P &#x3D; R的直线并求与各P-R曲线交点，一般认为BEP大的学习器性能好②F-score法：对P、R取调和平均，通过改变权重从而表达对学习器的查全率或查准率的重视程度(F1 度量是权重相同的F-score方法) “macro-P、macro-R;micro-P、micro-R”的区别：macro先在各个混淆矩阵上计算查全查准率后计算平均，micro先计算整体的正负样例比后计算整体的查全查准率","categories":[],"tags":[]},{"title":"testphoto","slug":"testphoto","date":"2022-09-13T16:33:48.000Z","updated":"2022-09-13T16:34:55.124Z","comments":true,"path":"2022/09/14/testphoto/","link":"","permalink":"http://example.com/2022/09/14/testphoto/","excerpt":"","text":"","categories":[],"tags":[]},{"title":"Mysql第一次作业","slug":"mysql01","date":"2022-09-13T16:21:04.128Z","updated":"2023-02-14T07:47:38.401Z","comments":true,"path":"2022/09/14/mysql01/","link":"","permalink":"http://example.com/2022/09/14/mysql01/","excerpt":"","text":"第一次有些仓促，先记录一下作业 作业完成时间——2022&#x2F;09&#x2F;14练习题 1.1 编写一条 CREATE TABLE 语句，用来创建一个包含表 1-A 中所列各项的表 Addressbook （地址簿），并为 regist_no （注册编号）列设置主键约束 我的解答： 1234567891011CREATE TABLE Addressbook(regist_no INTEGER NOT NULL,name VARCHAR(128) NOT NULL,address VARCHAR(256) NOT NULL,tel_no CHAR(10),mail_address CHAR(20),PRIMARY KEY (regist_no));INSERT INTO addressbook VALUES (&#x27;001&#x27;,&#x27;李华&#x27;,&#x27;北京市&#x27;,&#x27;1332222000&#x27;,&#x27;1234567890@qq.com&#x27;), (&#x27;002&#x27;, &#x27;张三&#x27;,&#x27;福建省&#x27;,&#x27;1352222000&#x27;,&#x27;987654321@163.com&#x27;), (&#x27;003&#x27;,&#x27;李四&#x27;,&#x27;四川省&#x27;,&#x27;1380000222&#x27;,&#x27;50505055050@126.com&#x27;); 结果展示： 1.2假设在创建练习1.1中的 Addressbook 表时忘记添加如下一列 postal_code （邮政编码）了，请编写 SQL 把此列添加到 Addressbook 表中。 列名 ： postal_code 数据类型 ：定长字符串类型（长度为 8） 约束 ：不能为 NULL 我的解答： 1ALTER TABLE addressbook ADD COLUMN postal_code CHAR(8) NOT NULL; 结果展示： 1.3请补充如下 SQL 语句来删除 Addressbook 表。( ) table Addressbook; 我的解答：DROP或TRUNCATE或DELETE，其中TRUNCATE清除数据速度最快效率最高。 1.4 是否可以编写 SQL 语句来恢复删除掉的Addressbook 表？ 我的解答：可以，但好像很麻烦 More info: 77’s git","categories":[],"tags":[]},{"title":"re+preview","slug":"preview","date":"2022-08-21T16:15:57.000Z","updated":"2022-08-21T17:15:49.631Z","comments":true,"path":"2022/08/22/preview/","link":"","permalink":"http://example.com/2022/08/22/preview/","excerpt":"","text":"关于深入浅出pytorch的一些记录—2022&#x2F;08&#x2F;22小总结datawhale的小组项目打卡已经完成了三次，想着总应该有一个小总结吧，在过去的 2+3+2&#x3D;&#x3D;7 天里，在colab上动手跟着教程敲代码搭建网络，虽然不敢说很理解，但也算是磕磕巴巴地入门了吧，相信这是一个好的开始~ 接下来，于此回顾一下前三期的内容: 我们波士顿动力小组希望后期多进行一些可视化训练,因此将学习进度稍向前提了一些，这三期的学习内容如下： day1~2: 复现fashionMINIST，进行数据预处理，了解数据标签的提取与加载 day3~5: 熟悉并掌握pytorch搭建神经网络框架 day4~7: 了解pytorch训练技巧 Day 1~2在第一阶段中，我们小组主要完成的任务是1.进行fashionMINIST的复现2.进行数据预处理(自己数据集的制作),以IFAR为例，主要的处理步骤为: ① 下载数据集② 下载解压后将数据集转换为图片并可视化检查结果是否正确③ 提取CIFAR的标签并建立字典进行图片标签的对应关系建立，分别并对于训练集和测试集分别生成两个json文件④ 试用自己的数据集进行简单的大小统一以及标签提取、数据集分割操作 Day 3~5第二次作业收获挺大，自我评估完成这次作业，也算小小入门了pytorch写深度学习框架:上一次作业，我们小组主要完成了数据集加载的学习，这次主要布置的任务承接上次，是做好数据集加载与训练模型的联系工作:这里主要学习到一个比较好用的方法transforms.Compose(),这个是pytorch中的图像预处理包。可以将多个步骤整合到一起，例如resize、CenterCrop ToTensor。由于之前只是在理论上对神经网络有一定理解，没有落实到实践中去，于是参考学习了资料中的模型构建到 pytorch优化器，比较系统地学习了神经网络各模块的 pytorch搭建方法，整理如下: ① 对于一个小模块来说，可以先定义一个类，继承nn.Module的属性，同时在构造函数中记得加上super(()函数来继承父类的方法，以便后续能为网络传入一个张量。② 模型的定义一般有几种方式:比较简明的是nn.Sequential()，这个方法保证sequetial内的层都是按顺序依次连接的，但是由于其实现的机理，这种方法的局限性是:灵活性低，一个模型定义好后要加入外部输入比较麻烦;另一种是ModuleList，这种方法是先将各层放入列表，并自动生成索引，在forward里按列表索引方式对各层进行连接即可，ModuleDict也类似，只是索引方式与字典相同。③ 模型建立之后要初始化，有很多初始化方式凯明初始化、常熟初始化、正态分布初始化等。④ 模型训练前要先选择合理的loss即损失函数，这里pytorch提供的选择明显比理论学习时学到的两种:一种交叉熵一种平方来的更多更灵活，但是具体什么情况下选择哪种损失函数更合理，这点或许值得深入。⑤ 注意训练和验证的小小不同:梯度是否回传，换言之权重是否会改变，训练改变而验证不改变，这符合常理。⑥ 注意优化器选择，尽管大部分时候选择简单的SGD但是还是有必要开拓下眼界。 最后，感觉整体不错，就是对于一些细节把握，由于大二上过一学期python，对于python理解还停留在表层因此在学习过程中，难免会要遇到一些问题很难理解，这时候多广泛查阅资料不断补足python知识，深入理解掌握本质，我想是很重要的，会对pytorch框架吃的更透。 day 6~7由于我们小组的进度会稍快些，因此模型的定义在前一次任务中已经学习完成了，在此就不再进行重复总结，这两天主要在学习pvtorch进阶训练技巧，有一些收获，但是还没落实到具体模型上去检验，现总结如下: ① 了解了可以通过自定义损失函数来构造和模型更匹配的损失承数例如:DiceLoss的构造，要注意的是，loss承数要继承自nnmodule才能够保证其能作为神经网络中的其中一层，参与到网络的前向传播的结果生成以及后向传播的初始函数提供。② 学习率是能够动态地进行调整的，注意调整这一步一般在优化器参数更新之后进而调整。③ 当我们想固定某一层，例如resnet50时，可以调用官方给的API即set parameter requires grad(model feature extract)来冻结model使之参数不进行反向传播④ timm库实际上可以视为对torch的拓展，在里头可以获得一些预训练模型直接调用timm.listmodels()传入想查询的模型如resnetdensenet可以查询到其提供的这个系列的所有模型，与torch一样能够以字典形式访问到某一层的参数列表，要更改模型输出也较方便，numclasses赋值为类别数即可。⑤ 了解到半精度训练:用16字节的浮点数替代32位的浮点数，因为事实上很多时候小数位不必很精确，这么做反而能节省显存，将空余的显存更多分配给需要的地方如调大 batch size,同时更低的精度意味着更快的运算速度，但是在数据size大的时候加速比较明显，这里有一个比较骚的表达:@autocast加在前向传播函数的定义前，即可指定此函数用半精度计算。⑥ imageaug可用于数据增强。⑦ argparser似乎是一个很高效的、能用来管理超参数的库，我们一般会定义一个config文件，在其中创建一个 get options函数，在其中调用parseraddargument来为不同的超参数指定类型和大小，只需要在train文件中调用这个承数，将这些参数传入即可，这样的好外在干，这些 小预告 在这里总结加预告一下，明天将是第一次正式用本blog记笔记（不懂算不算一个flaghh） 以及明天的内容将会是：pytorch可视化","categories":[],"tags":[]},{"title":"hello","slug":"hello","date":"2022-08-20T03:17:03.000Z","updated":"2022-11-09T13:42:32.212Z","comments":true,"path":"2022/08/20/hello/","link":"","permalink":"http://example.com/2022/08/20/hello/","excerpt":"","text":"","categories":[],"tags":[{"name":"others","slug":"others","permalink":"http://example.com/tags/others/"}]},{"title":"Welcome 2 my blog","slug":"hello-world","date":"2022-08-20T02:45:54.684Z","updated":"2022-08-20T15:24:59.451Z","comments":true,"path":"2022/08/20/hello-world/","link":"","permalink":"http://example.com/2022/08/20/hello-world/","excerpt":"","text":"Welcome to Luka’s zone! Actually it’s my 1st time to own a private blog! luka博客的诞生——2022&#x2F;08&#x2F;20起源为什么想要搭建本站？ 受到来自yangli大佬炫酷的博客启发 入坑深度学习，却苦于无处整合笔记 C盘快满了放不下notebook(假的) … 用途 本站将首要用于学习笔记存放，以便后续复习查阅 方便和水友交流(bushi) 见证mopagunda的学习历程(说得好) … 关于 自建立起，本博客将持续进行优化施工，以获得更好的用户体验 写在最后1234Hello,bro!Welcome to my blog.Hope u have a nice trip here~let us say it together:print(&quot;hello,my blog!&quot;) More info: 77’s git","categories":[],"tags":[]}],"categories":[],"tags":[{"name":"others","slug":"others","permalink":"http://example.com/tags/others/"}]}