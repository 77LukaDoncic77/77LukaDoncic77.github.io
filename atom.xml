<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>Hexo</title>
  
  
  <link href="http://example.com/atom.xml" rel="self"/>
  
  <link href="http://example.com/"/>
  <updated>2023-02-14T15:00:36.248Z</updated>
  <id>http://example.com/</id>
  
  <author>
    <name>John Doe</name>
    
  </author>
  
  <generator uri="https://hexo.io/">Hexo</generator>
  
  <entry>
    <title>20230214GNN</title>
    <link href="http://example.com/2023/02/14/20230214GNN/"/>
    <id>http://example.com/2023/02/14/20230214GNN/</id>
    <published>2023-02-14T07:43:05.000Z</published>
    <updated>2023-02-14T15:00:36.248Z</updated>
    
    <content type="html"><![CDATA[<h2 id="Task01：图机器学习导论"><a href="#Task01：图机器学习导论" class="headerlink" title="Task01：图机器学习导论"></a>Task01：图机器学习导论</h2><h3 id="1图嵌入"><a href="#1图嵌入" class="headerlink" title="1图嵌入"></a>1图嵌入</h3><h4 id="概念"><a href="#概念" class="headerlink" title="概念"></a>概念</h4><p>·图嵌入是一种将数据(通常为高位稠密的矩阵)映射为低维稠密向量的过程。</p><h3 id="2课程概述"><a href="#2课程概述" class="headerlink" title="2课程概述"></a>2课程概述</h3><blockquote><p>传统方法：Graghlets, Gragh Kernels<br>节点嵌入方法：DeepWalk, Node2Vec<br>图神经网络:GCN, GraghSAGE, GAT, Theory of GNNs<br>知识图谱：TransE, BetaE<br>图深度生成模型: GraghRNN </p></blockquote><h3 id="3图机器学习、图神经网络编程工具推荐"><a href="#3图机器学习、图神经网络编程工具推荐" class="headerlink" title="3图机器学习、图神经网络编程工具推荐"></a>3图机器学习、图神经网络编程工具推荐</h3><h4 id="网络搭建"><a href="#网络搭建" class="headerlink" title="网络搭建"></a>网络搭建</h4><blockquote><p>PyG<br>NetworkX<br>DGL</p></blockquote><h4 id="网络可视化"><a href="#网络可视化" class="headerlink" title="网络可视化"></a>网络可视化</h4><blockquote><p>AntV<br>Echarts</p></blockquote><h4 id="图数据库"><a href="#图数据库" class="headerlink" title="图数据库"></a>图数据库</h4><blockquote><p>Neo4j</p></blockquote><h3 id="4图机器学习应用"><a href="#4图机器学习应用" class="headerlink" title="4图机器学习应用"></a>4图机器学习应用</h3><blockquote><p>Pathfinding<br>Cetrality&#x2F;importance<br>Community Detection<br>Link Prediction<br>Similarity<br>Embeddings   </p></blockquote><h3 id="5不同层次的任务"><a href="#5不同层次的任务" class="headerlink" title="5不同层次的任务"></a>5不同层次的任务</h3><blockquote><p>Node level<br>Edge level<br>Community level<br>Gragh level  </p></blockquote><h4 id="Example"><a href="#Example" class="headerlink" title="Example"></a>Example</h4><blockquote><p>Node level已知节点类别去推断未知节点类别<br>Edge level[Recommender Systems] 已知连接去推断位置连接  </p></blockquote><h3 id="升华"><a href="#升华" class="headerlink" title="升华"></a>升华</h3><p>“图是最优质的长期资产”、”网络效应是一个企业最深的护城河”</p><h3 id="几个图数据挖掘的项目"><a href="#几个图数据挖掘的项目" class="headerlink" title="几个图数据挖掘的项目"></a>几个图数据挖掘的项目</h3><blockquote><p>readpaper.com 本质上是大的文献图<br>connected papers 也是文献图<br>BIOS 医疗知识图谱 3000w+篇医学文献<br>liuhuanyong.github.io 刘焕勇 知识图谱大佬<br>Hypercrx浏览器插件<br>OpenRank 定量计算开源项目活跃度<br>open-galaxy.com</p></blockquote><h3 id="思考题"><a href="#思考题" class="headerlink" title="思考题"></a>思考题</h3><h4 id="A股、港股、美股市值最高的上市公司，哪些公司的核心资产是图？"><a href="#A股、港股、美股市值最高的上市公司，哪些公司的核心资产是图？" class="headerlink" title="A股、港股、美股市值最高的上市公司，哪些公司的核心资产是图？"></a>A股、港股、美股市值最高的上市公司，哪些公司的核心资产是图？</h4><p>答：腾讯、阿里巴巴、美团的核心资产是图   </p><h4 id="观看电影《社交网络》，图和图数据挖掘的商业价值体现在哪些方面？"><a href="#观看电影《社交网络》，图和图数据挖掘的商业价值体现在哪些方面？" class="headerlink" title="观看电影《社交网络》，图和图数据挖掘的商业价值体现在哪些方面？"></a>观看电影《社交网络》，图和图数据挖掘的商业价值体现在哪些方面？</h4><p>答：没有看过《社交网络》，但是图和图数据挖掘的商业价值体现在：<br>1.能够进行社交网络分析：可以帮助企业了解客户、市场和竞争对手的关系，识别潜在的合作伙伴和客户，以改善营销策略<br>2.风险管理：可以帮助企业识别潜在的风险和漏洞，以及简历更准确的风险模型，从而帮助企业降低风险和损失<br>3.推荐系统：可以基于用户之间的相似性和关系，为用户提供个性化的推荐服务，提高用户满意度和忠诚度。<br>4.物流和供应链管理：可以帮助企业更好地理解和管理供应链中的复杂关系和流程，从而提高效率和降低成本<br>5.健康医疗：可以帮助医疗机构更好地识别病人之间的关系，预测病情发展趋势，以及优化医疗资源分配   </p><h4 id="图神经网络和其它神经网络有什么区别？"><a href="#图神经网络和其它神经网络有什么区别？" class="headerlink" title="图神经网络和其它神经网络有什么区别？"></a>图神经网络和其它神经网络有什么区别？</h4><p>图神经网络是一类专门用于处理图数据的神经网络模型，相对与其他神经网络，主要有以下几点区别<br>1.输入数据结构不同：图神经网络的输入是图结构，即由节点和边构成的复杂数据结构，而其它神经网络的输入通常是向量、矩阵或张量等简单的数据结构。<br>2.节点特征的自适应聚合：图神经网络能够通过节点的自适应聚合，将节点特征进行局部汇聚，以此获取全局特征，而其它神经网络则是通过卷积、池化等操作从全局特征中提取局部特征。<br>3.对称性：在处理图数据时，由于图结构的不对称性，GNN需要考虑不同节点和边的关系，而其它神经网络中往往没有对称性的考虑。<br>4.更强的表示能力：相比于传统神经网络，GNN在处理图数据时具有更强的表示能力，能够对节点和边进行更细粒度的建模，因此在图数据相关的任务上通常能够取得更好的性能。   </p><hr><p>参考文献：<br><a href="https://chat.openai.com/chat">https://chat.openai.com/chat</a><br><a href="https://github.com/TommyZihao/zihao_course/blob/main/CS224W/1-Intro.md">https://github.com/TommyZihao/zihao_course/blob/main/CS224W/1-Intro.md</a><br><a href="https://new.qq.com/rain/a/20221228A06Q9G00">https://new.qq.com/rain/a/20221228A06Q9G00</a><br><a href="https://www.bilibili.com/video/BV1pR4y1S7GA/?vd_source=e966406f0253d32d0fcdc902c479718a">https://www.bilibili.com/video/BV1pR4y1S7GA/?vd_source=e966406f0253d32d0fcdc902c479718a</a></p><p><strong>保持开放和学习 创造更多的连接和惊喜</strong></p>]]></content>
    
    
      
      
    <summary type="html">&lt;h2 id=&quot;Task01：图机器学习导论&quot;&gt;&lt;a href=&quot;#Task01：图机器学习导论&quot; class=&quot;headerlink&quot; title=&quot;Task01：图机器学习导论&quot;&gt;&lt;/a&gt;Task01：图机器学习导论&lt;/h2&gt;&lt;h3 id=&quot;1图嵌入&quot;&gt;&lt;a href=&quot;#1</summary>
      
    
    
    
    
  </entry>
  
  <entry>
    <title>20221011ML</title>
    <link href="http://example.com/2022/10/11/20221011ML/"/>
    <id>http://example.com/2022/10/11/20221011ML/</id>
    <published>2022-10-11T12:51:03.000Z</published>
    <updated>2022-10-11T15:35:31.878Z</updated>
    
    <content type="html"><![CDATA[<blockquote><p>很神奇的是打卡和课堂的ML课程在10.10号刚好同时开始嘿嘿</p></blockquote><h2 id="Task01：概览西瓜书-南瓜书第1、2章"><a href="#Task01：概览西瓜书-南瓜书第1、2章" class="headerlink" title="Task01：概览西瓜书 + 南瓜书第1、2章"></a>Task01：概览西瓜书 + 南瓜书第1、2章</h2><h3 id="Chap01-ML的一些基本术语"><a href="#Chap01-ML的一些基本术语" class="headerlink" title="Chap01 ML的一些基本术语"></a>Chap01 ML的一些基本术语</h3><p>数据集、标记、训练样本、测试样本、正类和负类、分布、模型泛化能力…这些概念虽然很抽象但是带入具体例子就一目了然因此不深究</p><h3 id="什么是归纳偏好？"><a href="#什么是归纳偏好？" class="headerlink" title="什么是归纳偏好？"></a>什么是归纳偏好？</h3><p>对于一组训练样本，在多数情况下有多种模型可供选择，且不同的模型往往对应不同的输出效果，而一个具体的机器学习算法要求：“必须要产生一个模型”以使得对于一个给定的输入，有一个稳定的输出，因此我们必须要选择一个模型，这时候所选择的算法在实际输出时就会表现出某种偏好，称之为归纳偏好。</p><h4 id="Ocaam’s-razor？"><a href="#Ocaam’s-razor？" class="headerlink" title="Ocaam’s razor？"></a>Ocaam’s razor？</h4><p>由于可以不同模型有不同的偏好，那实际生活中我们需要有一种偏好选择的基准，<em>奥卡姆剃刀</em>为一种常用的、自然科学研究中的基本准则。</p><h4 id="NFL定理"><a href="#NFL定理" class="headerlink" title="NFL定理"></a>NFL定理</h4><p>通过理论可以证明（参见pumpkin book P1），对于二分类的总误差与算法选择并无关联，他们的总体期望性能是相同的，这意味着，对于拥有任何可能分布的庞大数据集来说，用模型估计和随机猜测的误差会几乎相同，但这是否意味着我们设计模型没有意义呢？并非如此~事实上，我们设计模型的初衷总是针对一个特定的任务，而这个任务的数据分布往往是一定的，因此训练的模型往往能够鲁棒地在上面发挥作用。套用机器学习老师的说法：“一切抛开具体问题、具体数据分布的模型性能讨论都是在耍流氓[doge]”</p><hr><h3 id="Chap02-模型评估与选择"><a href="#Chap02-模型评估与选择" class="headerlink" title="Chap02 模型评估与选择"></a>Chap02 模型评估与选择</h3><h4 id="概念"><a href="#概念" class="headerlink" title="概念"></a>概念</h4><p>经验误差、泛化误差、过拟合</p><blockquote><p>经验误差：学习器在训练集上的误差<br>泛化误差：新样本上的误差称为“泛化误差”<br>过拟合：学习能力过于强大，以至于把训练样本所包含的不太一般的特性都学到了</p></blockquote><h4 id="评估方法"><a href="#评估方法" class="headerlink" title="评估方法"></a>评估方法</h4><blockquote><p>可以通过实验测试来对学习器泛化误差进行评估,从而做出选择</p></blockquote><p>way1:hold-out<br>该方法普遍使用于数据集比较大的情况:<br>留出法暴力&amp;&amp;好理解:将数据集划分为两个互斥的集合<br>注意:①留出法本质是一种分层抽样,要严格保证训练样本&#x2F;测试样本的数据分布与原数据集相同②要注意数据集划分比例2:8&#x2F;3:7势必会使训练出模型的性能不同的,选择比例的时候要斟酌</p><p>way2:交叉验证法<br>将数据集划分为k个大小相似的互斥子集,每个子集也都同时尽可能保持分布一致性(似乎在k值和样本数相接近时,这一点较难保证),进而进行k次的训练和测试.</p><p>way3:自助法<br>采用有放回抽样的方法,保证训练集的规模和原数据集D一致,但其中样本并不完备,事实上,D中有36.8%左右样本在采样后数据集中不会出现(一个常用极限)</p><h4 id="性能度量"><a href="#性能度量" class="headerlink" title="性能度量"></a>性能度量</h4><blockquote><ul><li>错误率: $E(f;D) &#x3D; \frac{1}{m}\sum_{i&#x3D;1}^mII(f(x_i)!&#x3D;y_i)$</li><li>精度: $acc(f;D) &#x3D; 1 - E(f;D)$</li><li>查准率:$P &#x3D; \frac{TP}{TP + FP}$, 查全率$R &#x3D; \frac{FN}{FN + TN}$</li></ul></blockquote><blockquote><ul><li>“P-R曲线”</li></ul></blockquote><p><img src="/2022/10/11/20221011ML/precision&&recall.jpg" alt="P-R曲线"></p><p>作图方法：根据学习器的预测结果对样例进行排序，排在在前面的是预测的可能性最大的正例样本，按照这个顺序逐个将样本作为正例进行预测，每次计算出当前的查全率、查准率，并绘制P-R曲线<br>作用：直观显示出学习器在样本总体上的查准率、查全率———最优的P-R曲线曲线下面积为1<br>学习器效果对比：<br>①利用BEP(Break-Even Point)就是在P-R图上做一条P &#x3D; R的直线并求与各P-R曲线交点，一般认为BEP大的学习器性能好<br>②F-score法：对P、R取调和平均，通过改变权重从而表达对学习器的查全率或查准率的重视程度(F1 度量是权重相同的F-score方法)</p><blockquote><ul><li>“macro-P、macro-R;micro-P、micro-R”的区别：macro先在各个混淆矩阵上计算查全查准率后计算平均，micro先计算整体的正负样例比后计算整体的查全查准率</li></ul></blockquote>]]></content>
    
    
      
      
    <summary type="html">&lt;blockquote&gt;
&lt;p&gt;很神奇的是打卡和课堂的ML课程在10.10号刚好同时开始嘿嘿&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h2 id=&quot;Task01：概览西瓜书-南瓜书第1、2章&quot;&gt;&lt;a href=&quot;#Task01：概览西瓜书-南瓜书第1、2章&quot; class=&quot;head</summary>
      
    
    
    
    
  </entry>
  
  <entry>
    <title>testphoto</title>
    <link href="http://example.com/2022/09/14/testphoto/"/>
    <id>http://example.com/2022/09/14/testphoto/</id>
    <published>2022-09-13T16:33:48.000Z</published>
    <updated>2022-09-13T16:34:55.124Z</updated>
    
    <content type="html"><![CDATA[<p><img src="/2022/09/14/testphoto/lane.jpg" alt="lane"></p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;&lt;img src=&quot;/2022/09/14/testphoto/lane.jpg&quot; alt=&quot;lane&quot;&gt;&lt;/p&gt;
</summary>
      
    
    
    
    
  </entry>
  
  <entry>
    <title>Mysql第一次作业</title>
    <link href="http://example.com/2022/09/14/mysql01/"/>
    <id>http://example.com/2022/09/14/mysql01/</id>
    <published>2022-09-13T16:21:04.128Z</published>
    <updated>2023-02-14T07:47:38.401Z</updated>
    
    <content type="html"><![CDATA[<p><em>第一次有些仓促，先记录一下作业</em></p><h2 id="作业完成时间——2022-x2F-09-x2F-14"><a href="#作业完成时间——2022-x2F-09-x2F-14" class="headerlink" title="作业完成时间——2022&#x2F;09&#x2F;14"></a>作业完成时间——2022&#x2F;09&#x2F;14</h2><h3 id="练习题"><a href="#练习题" class="headerlink" title="练习题"></a>练习题</h3><blockquote><p>1.1 编写一条 CREATE TABLE 语句，用来创建一个包含表 1-A 中所列各项的表 Addressbook （地址簿），并为 regist_no （注册编号）列设置主键约束</p></blockquote><p><img src="/2022/09/14/mysql01/xiti1.1.png" alt="xiti1.1"></p><p>我的解答：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">CREATE TABLE Addressbook</span><br><span class="line">(regist_no INTEGER NOT NULL,</span><br><span class="line">name VARCHAR(128) NOT NULL,</span><br><span class="line">address VARCHAR(256) NOT NULL,</span><br><span class="line">tel_no CHAR(10),</span><br><span class="line">mail_address CHAR(20),</span><br><span class="line">PRIMARY KEY (regist_no));</span><br><span class="line"></span><br><span class="line">INSERT INTO addressbook VALUES (<span class="string">&#x27;001&#x27;</span>,<span class="string">&#x27;李华&#x27;</span>,<span class="string">&#x27;北京市&#x27;</span>,<span class="string">&#x27;1332222000&#x27;</span>,<span class="string">&#x27;1234567890@qq.com&#x27;</span>),</span><br><span class="line">(<span class="string">&#x27;002&#x27;</span>, <span class="string">&#x27;张三&#x27;</span>,<span class="string">&#x27;福建省&#x27;</span>,<span class="string">&#x27;1352222000&#x27;</span>,<span class="string">&#x27;987654321@163.com&#x27;</span>),</span><br><span class="line">(<span class="string">&#x27;003&#x27;</span>,<span class="string">&#x27;李四&#x27;</span>,<span class="string">&#x27;四川省&#x27;</span>,<span class="string">&#x27;1380000222&#x27;</span>,<span class="string">&#x27;50505055050@126.com&#x27;</span>);</span><br></pre></td></tr></table></figure><p>结果展示：<br><img src="/2022/09/14/mysql01/zuoye1.1.jpg" alt="zuoye1"></p><hr><blockquote><p>1.2假设在创建练习1.1中的 Addressbook 表时忘记添加如下一列 postal_code （邮政编码）了，请编写 SQL 把此列添加到 Addressbook 表中。</p><ul><li><em>列名 ： postal_code</em></li><li><em>数据类型 ：定长字符串类型（长度为 8）</em></li><li><em>约束 ：不能为 NULL</em></li></ul></blockquote><p>我的解答：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">ALTER TABLE addressbook ADD COLUMN postal_code CHAR(8) NOT NULL;</span><br></pre></td></tr></table></figure><p>结果展示：<br><img src="/2022/09/14/mysql01/zuoye1.2.png" alt="zuoye2"></p><hr><blockquote><p>1.3请补充如下 SQL 语句来删除 Addressbook 表。<br>(    ) table Addressbook;</p></blockquote><p>我的解答：DROP或TRUNCATE或DELETE，其中TRUNCATE清除数据速度最快效率最高。</p><blockquote><p>1.4 是否可以编写 SQL 语句来恢复删除掉的<br>Addressbook 表？</p></blockquote><p>我的解答：可以，但好像很麻烦</p><hr><p>More info: <a href="https://github.com/77LukaDoncic77">77’s git</a></p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;&lt;em&gt;第一次有些仓促，先记录一下作业&lt;/em&gt;&lt;/p&gt;
&lt;h2 id=&quot;作业完成时间——2022-x2F-09-x2F-14&quot;&gt;&lt;a href=&quot;#作业完成时间——2022-x2F-09-x2F-14&quot; class=&quot;headerlink&quot; title=&quot;作业完成时间——</summary>
      
    
    
    
    
  </entry>
  
  <entry>
    <title>re+preview</title>
    <link href="http://example.com/2022/08/22/preview/"/>
    <id>http://example.com/2022/08/22/preview/</id>
    <published>2022-08-21T16:15:57.000Z</published>
    <updated>2022-08-21T17:15:49.631Z</updated>
    
    <content type="html"><![CDATA[<h1 id="关于深入浅出pytorch的一些记录—2022-x2F-08-x2F-22"><a href="#关于深入浅出pytorch的一些记录—2022-x2F-08-x2F-22" class="headerlink" title="关于深入浅出pytorch的一些记录—2022&#x2F;08&#x2F;22"></a>关于深入浅出pytorch的一些记录—2022&#x2F;08&#x2F;22</h1><h3 id="小总结"><a href="#小总结" class="headerlink" title="小总结"></a>小总结</h3><p>datawhale的小组项目打卡已经完成了三次，想着总应该有一个小总结吧，在过去的 <em><strong>2+3+2&#x3D;&#x3D;7</strong></em> 天里，在colab上动手跟着教程敲代码搭建网络，虽然不敢说很理解，但也算是磕磕巴巴地入门了吧，相信这是一个好的开始~</p><p>接下来，于此回顾一下前三期的内容:</p><p>我们波士顿动力小组希望后期多进行一些可视化训练,因此将学习进度稍向前提了一些，这三期的学习内容如下：</p><blockquote><ul><li><strong>day1~2:</strong> 复现fashionMINIST，进行数据预处理，了解数据标签的提取与加载</li><li><strong>day3~5:</strong> 熟悉并掌握pytorch搭建神经网络框架</li><li><strong>day4~7:</strong> 了解pytorch训练技巧</li></ul></blockquote><hr><h4 id="Day-1-2"><a href="#Day-1-2" class="headerlink" title="Day 1~2"></a>Day 1~2</h4><p>在第一阶段中，我们小组主要完成的任务是<br>1.进行fashionMINIST的复现<br>2.进行数据预处理(自己数据集的制作),以IFAR为例，主要的处理步骤为:</p><blockquote><p>① 下载数据集<br>② 下载解压后将数据集转换为图片并可视化检查结果是否正确<br>③ 提取CIFAR的标签并建立字典进行图片标签的对应关系建立，分别并对于训练集和测试集分别生成两个json文件<br>④ 试用自己的数据集进行简单的大小统一以及标签提取、数据集分割操作</p></blockquote><h4 id="Day-3-5"><a href="#Day-3-5" class="headerlink" title="Day 3~5"></a>Day 3~5</h4><p>第二次作业收获挺大，自我评估完成这次作业，也算小小入门了pytorch写深度学习框架:上一次作业，我们小组主要完成了数据集加载的学习，这次主要布置的任务承接上次，是做好数据集加载与训练模型的联系工作:这里主要学习到一个比较好用的方法transforms.Compose(),这个是pytorch中的图像预处理包。可以将多个步骤整合到一起，例如resize、CenterCrop ToTensor。<br>由于之前只是在理论上对神经网络有一定理解，没有落实到实践中去，于是参考学习了资料中的模型构建到 pytorch优化器，比较系统地学习了神经网络各模块的 pytorch搭建方法，整理如下:</p><blockquote><p>① 对于一个小模块来说，可以先定义一个类，继承nn.Module的属性，同时在构造函数中记得加上super(()函数来继承父类的方法，以便后续能为网络传入一个张量。<br>② 模型的定义一般有几种方式:比较简明的是nn.Sequential()，这个方法保证sequetial内的层都是按顺序依次连接的，但是由于其实现的机理，这种方法的局限性是:灵活性低，一个模型定义好后要加入外部输入比较麻烦;另一种是ModuleList，这种方法是先将各层放入列表，并自动生成索引，在forward里按列表索引方式对各层进行连接即可，ModuleDict也类似，只是索引方式与字典相同。<br>③ 模型建立之后要初始化，有很多初始化方式凯明初始化、常熟初始化、正态分布初始化等。<br>④ 模型训练前要先选择合理的loss即损失函数，这里pytorch提供的选择明显比理论学习时学到的两种:一种交叉熵一种平方来的更多更灵活，但是具体什么情况下选择哪种损失函数更合理，这点或许值得深入。<br>⑤ 注意训练和验证的小小不同:梯度是否回传，换言之权重是否会改变，训练改变而验证不改变，这符合常理。<br>⑥ 注意优化器选择，尽管大部分时候选择简单的SGD但是还是有必要开拓下眼界。</p></blockquote><p>最后，感觉整体不错，就是对于一些细节把握，由于大二上过一学期python，对于python理解还停留在表层因此在学习过程中，难免会要遇到一些问题很难理解，这时候多广泛查阅资料不断补足python知识，深入理解掌握本质，我想是很重要的，会对pytorch框架吃的更透。</p><h4 id="day-6-7"><a href="#day-6-7" class="headerlink" title="day 6~7"></a>day 6~7</h4><p>由于我们小组的进度会稍快些，因此模型的定义在前一次任务中已经学习完成了，在此就不再进行重复总结，这两天主要在学习pvtorch进阶训练技巧，有一些收获，但是还没落实到具体模型上去检验，现总结如下:</p><blockquote><p>① 了解了可以通过自定义损失函数来构造和模型更匹配的损失承数例如:DiceLoss的构造，要注意的是，loss承数要继承自nnmodule才能够保证其能作为神经网络中的其中一层，参与到网络的前向传播的结果生成以及后向传播的初始函数提供。<br>② 学习率是能够动态地进行调整的，注意调整这一步一般在优化器参数更新之后进而调整。<br>③ 当我们想固定某一层，例如resnet50时，可以调用官方给的API即set parameter requires grad(model feature extract)来冻结model使之参数不进行反向传播<br>④ timm库实际上可以视为对torch的拓展，在里头可以获得一些预训练模型直接调用timm.listmodels()传入想查询的模型如resnetdensenet可以查询到其提供的这个系列的所有模型，与torch一样能够以字典形式访问到某一层的参数列表，要更改模型输出也较方便，numclasses赋值为类别数即可。<br>⑤ 了解到半精度训练:用16字节的浮点数替代32位的浮点数，因为事实上很多时候小数位不必很精确，这么做反而能节省显存，将空余的显存更多分配给需要的地方如调大 batch size,同时更低的精度意味着更快的运算速度，但是在数据size大的时候加速比较明显，这里有一个比较骚的表达:@autocast加在前向传播函数的定义前，即可指定此函数用半精度计算。<br>⑥ imageaug可用于数据增强。<br>⑦ argparser似乎是一个很高效的、能用来管理超参数的库，我们一般会定义一个config文件，在其中创建一个 get options函数，在其中调用parseraddargument来为不同的超参数指定类型和大小，只需要在train文件中调用这个承数，将这些参数传入即可，这样的好外在干，这些</p></blockquote><hr><h3 id="小预告"><a href="#小预告" class="headerlink" title="小预告"></a>小预告</h3><blockquote><ul><li>在这里总结加预告一下，明天将是第一次正式用本blog记笔记（<em>不懂算不算一个flaghh</em>）</li><li>以及明天的内容将会是：<strong>pytorch可视化</strong></li></ul></blockquote>]]></content>
    
    
      
      
    <summary type="html">&lt;h1 id=&quot;关于深入浅出pytorch的一些记录—2022-x2F-08-x2F-22&quot;&gt;&lt;a href=&quot;#关于深入浅出pytorch的一些记录—2022-x2F-08-x2F-22&quot; class=&quot;headerlink&quot; title=&quot;关于深入浅出pytorch的一些记录</summary>
      
    
    
    
    
  </entry>
  
  <entry>
    <title>hello</title>
    <link href="http://example.com/2022/08/20/hello/"/>
    <id>http://example.com/2022/08/20/hello/</id>
    <published>2022-08-20T03:17:03.000Z</published>
    <updated>2022-11-09T13:42:32.212Z</updated>
    
    
    
    
    
    <category term="others" scheme="http://example.com/tags/others/"/>
    
  </entry>
  
  <entry>
    <title>Welcome 2 my blog</title>
    <link href="http://example.com/2022/08/20/hello-world/"/>
    <id>http://example.com/2022/08/20/hello-world/</id>
    <published>2022-08-20T02:45:54.684Z</published>
    <updated>2022-08-20T15:24:59.451Z</updated>
    
    <content type="html"><![CDATA[<p>Welcome to <a href="https://77lukadoncic77.github.io/">Luka’s zone</a>! </p><p>Actually it’s my 1st time to own a private blog!</p><!-- If you get any problems when using Hexo, you can find the answer in [troubleshooting](https://hexo.io/docs/troubleshooting.html) or you can ask me on [GitHub](https://github.com/hexojs/hexo/issues). --><h2 id="luka博客的诞生——2022-x2F-08-x2F-20"><a href="#luka博客的诞生——2022-x2F-08-x2F-20" class="headerlink" title="luka博客的诞生——2022&#x2F;08&#x2F;20"></a>luka博客的诞生——2022&#x2F;08&#x2F;20</h2><h3 id="起源"><a href="#起源" class="headerlink" title="起源"></a>起源</h3><p><br><strong>为什么想要搭建本站？</strong><br><br></p><blockquote><ul><li>受到来自<a href="https://yangli-os.github.io/">yangli</a>大佬炫酷的博客启发</li><li>入坑深度学习，却苦于无处整合笔记</li><li>C盘快满了放不下notebook(<em>假的</em>)</li><li>…</li></ul></blockquote><hr><h3 id="用途"><a href="#用途" class="headerlink" title="用途"></a>用途</h3><blockquote><ul><li>本站将首要用于学习笔记存放，以便后续复习查阅</li><li>方便和水友交流(<em>bushi</em>)</li><li>见证mopagunda的学习历程(<em><strong>说得好</strong></em>)</li><li>…</li></ul></blockquote><hr><h3 id="关于"><a href="#关于" class="headerlink" title="关于"></a>关于</h3><blockquote><ul><li>自建立起，本博客将<strong>持续</strong>进行<strong>优化施工</strong>，以获得更好的<strong>用户体验</strong></li></ul></blockquote><hr><h3 id="写在最后"><a href="#写在最后" class="headerlink" title="写在最后"></a>写在最后</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">Hello,bro!Welcome to my blog.</span><br><span class="line">Hope u have a <span class="built_in">nice</span> trip here~</span><br><span class="line"><span class="built_in">let</span> us say it together:</span><br><span class="line"><span class="built_in">print</span>(<span class="string">&quot;hello,my blog!&quot;</span>)</span><br></pre></td></tr></table></figure><p>More info: <a href="https://github.com/77LukaDoncic77">77’s git</a></p>]]></content>
    
    
      
      
    <summary type="html">&lt;p&gt;Welcome to &lt;a href=&quot;https://77lukadoncic77.github.io/&quot;&gt;Luka’s zone&lt;/a&gt;! &lt;/p&gt;
&lt;p&gt;Actually it’s my 1st time to own a private blog!&lt;/p&gt;
&lt;!--</summary>
      
    
    
    
    
  </entry>
  
</feed>
